{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Persian Wikipedia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initialize configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = \"Configs/config_fa.yml\"  # Persian config\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Initialize phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52b79ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udce5 Loading dictionary files from: /root/Persian-PL-BERT/.venv/lib/python3.12/site-packages/vaguye/persian-dict\n",
      "\u2705 Loaded persian-primary.json\n",
      "\u2705 Loaded persian-secondary.json\n",
      "\ud83d\udcda Total entries loaded: 64183\n"
     ]
    }
   ],
   "source": [
    "from phonemize_fa import phonemize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d58c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Persian-PL-BERT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config['dataset_params']['tokenizer'])  # Persian BERT tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25418",
   "metadata": {},
   "source": [
    "### Load Persian Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Persian Wikipedia from ./datasets/wikipedia-fa.txt...\n",
      "Loaded 104122 lines from Persian Wikipedia\n",
      "Created dataset with 104122 examples\n"
     ]
    }
   ],
   "source": [
    "from load_persian_dataset import load_persian_wikipedia\n",
    "dataset = load_persian_wikipedia(\"./datasets/wikipedia-fa.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ca2f4",
   "metadata": {},
   "source": [
    "### Setup multiprocessing for dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca7ca2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"./wiki_phoneme_fa\"  # set up root directory for multiprocessor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a578d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_shards = 2000  # Adjusted for 1.3M lines (approx 650 lines per shard)\n",
    "\n",
    "def process_shard(i):\n",
    "    directory = root_directory + \"/shard_\" + str(i)\n",
    "    if os.path.exists(directory):\n",
    "        print(\"Shard %d already exists!\" % i)\n",
    "        return\n",
    "    print('Processing shard %d ...' % i)\n",
    "    shard = dataset.shard(num_shards=num_shards, index=i)\n",
    "    processed_dataset = shard.map(lambda t: phonemize(t['text'], tokenizer), remove_columns=['text'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    processed_dataset.save_to_disk(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73caf6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pebble'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpebble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProcessPool\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pebble'"
     ]
    }
   ],
   "source": [
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f9dcf",
   "metadata": {},
   "source": [
    "#### Note: You may need to run the following cell multiple times to process all shards because some will fail. Depending on how fast you process each shard, you may need to change the timeout to a longer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_workers = 32  # change this to the number of CPU cores your machine has\n",
    "\n",
    "with ProcessPool(max_workers=max_workers) as pool:\n",
    "    # Increased timeout to 300s (5 mins) to be safe\n",
    "    future = pool.map(process_shard, range(num_shards), timeout=300)\n",
    "    \n",
    "    iterator = iter(future)\n",
    "    while True:\n",
    "        try:\n",
    "            next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except TimeoutError as error:\n",
    "            print(\"Shard processing timed out\")\n",
    "        except Exception as error:\n",
    "            print(f\"Shard processing failed: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "output = [dI for dI in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory,dI))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = root_directory + \"/\" + o\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        print(\"%s loaded\" % o)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset.save_to_disk(config['data_folder'])\n",
    "print('Dataset saved to %s' % config['data_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Generate token maps for Persian vocabulary\n",
    "We need to create mappings from BERT token IDs to reduced vocabulary for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "file_data = FilePathDataset(dataset)\n",
    "loader = build_dataloader(file_data, num_workers=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique tokens in the entire dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_index = [special_token]\n",
    "for _, batch in enumerate(tqdm(loader)):\n",
    "    unique_index.extend(batch)\n",
    "    unique_index = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Persian BERT, we don't need to lowercase (parsbert is already uncased)\n",
    "# Just create direct mapping\n",
    "\n",
    "lower_tokens = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mapping for Persian tokens\n",
    "\n",
    "token_maps = {}\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    token_maps[t] = {'word': word, 'token': lower_tokens.index(t)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c94be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "print('Token mapper saved to %s' % config['dataset_params']['token_maps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=32, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70874215",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}