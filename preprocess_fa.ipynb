{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Persian Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86397816",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pebble -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initialize configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = \"Configs/config_fa.yml\"  # Persian config\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Initialize phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa110b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://git@github.com/SadeghKrmi/pernorm.git\n",
    "# !pip install git+https://git@github.com/SadeghKrmi/zirneshane.git\n",
    "# !pip install git+https://git@github.com/SadeghKrmi/vaguye.git\n",
    "# !pip install git+https://git@github.com/SadeghKrmi/hamnevise.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52b79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemize_fa import phonemize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d58c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config['dataset_params']['tokenizer'])  # Persian BERT tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25418",
   "metadata": {},
   "source": [
    "### Load Persian Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Persian Wikipedia from ./datasets/wikipedia-fa-cleaned-samples.txt...\n",
      "Loaded 258 lines from Persian Wikipedia\n",
      "Created dataset with 258 examples\n"
     ]
    }
   ],
   "source": [
    "from load_persian_dataset import load_persian_wikipedia\n",
    "dataset = load_persian_wikipedia(\"./datasets/wikipedia-fa-cleaned-samples.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ca2f4",
   "metadata": {},
   "source": [
    "### Setup multiprocessing for dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca7ca2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"./datasets/wiki_phoneme_fa\"  # set up root directory for multiprocessor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a578d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_shards = 2000  # Adjusted for 1.3M lines (approx 650 lines per shard)\n",
    "\n",
    "def process_shard(i):\n",
    "    directory = root_directory + \"/shard_\" + str(i)\n",
    "    if os.path.exists(directory):\n",
    "        print(\"Shard %d already exists!\" % i)\n",
    "        return\n",
    "    print('Processing shard %d ...' % i)\n",
    "    shard = dataset.shard(num_shards=num_shards, index=i)\n",
    "    processed_dataset = shard.map(lambda t: phonemize(t['text'], tokenizer), remove_columns=['text'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    processed_dataset.save_to_disk(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73caf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f9dcf",
   "metadata": {},
   "source": [
    "#### Note: You may need to run the following cell multiple times to process all shards because some will fail. Depending on how fast you process each shard, you may need to change the timeout to a longer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_workers = 32  # change this to the number of CPU cores your machine has\n",
    "\n",
    "with ProcessPool(max_workers=max_workers) as pool:\n",
    "    # Increased timeout to 300s (5 mins) to be safe\n",
    "    future = pool.map(process_shard, range(num_shards), timeout=300)\n",
    "    for result in future.result():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0568da38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_3 loaded\n",
      "shard_15 loaded\n",
      "shard_11 loaded\n",
      "shard_5 loaded\n",
      "shard_9 loaded\n",
      "shard_17 loaded\n",
      "shard_0 loaded\n",
      "shard_13 loaded\n",
      "shard_10 loaded\n",
      "shard_16 loaded\n",
      "shard_18 loaded\n",
      "shard_19 loaded\n",
      "shard_1 loaded\n",
      "shard_14 loaded\n",
      "shard_12 loaded\n",
      "shard_2 loaded\n",
      "shard_7 loaded\n",
      "shard_6 loaded\n",
      "shard_8 loaded\n",
      "shard_4 loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "output = [dI for dI in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory,dI))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = root_directory + \"/\" + o\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        print(\"%s loaded\" % o)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d032cb791244550bca030d1c2442023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/258 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to wikipedia_fa.processed\n"
     ]
    }
   ],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset.save_to_disk(config['data_folder'])\n",
    "print('Dataset saved to %s' % config['data_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'phonemes'],\n",
       "    num_rows: 258\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41a72b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['viːkiːpedˈjɒː', 'tɒː', 'ˈdo', 'ordiːbeˈheʃt', 'ˈjek', 'heˈzɒːr', 'o', 'siːˈsæd', 'o', 'næˈvæd', 'o', 'ˈjek', 'drbrɡiːrndeˈje', 'deˈviːst', 'o', 'hæfˈtɒːd', 'o', 'ˈnæ', 'zæˈbɒːn', 'bɒː', 'ˌbiːʃ', 'æz', 'ˈdæh', 'mæqɒːˈleː', 'buːdeːˈˌæst', '.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1]['phonemes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Generate token maps for Persian vocabulary\n",
    "We need to create mappings from BERT token IDs to reduced vocabulary for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "file_data = FilePathDataset(dataset)\n",
    "loader = build_dataloader(file_data, num_workers=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b7504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fcb44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 41.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all unique tokens in the entire dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_index = [special_token]\n",
    "for _, batch in enumerate(tqdm(loader)):\n",
    "    unique_index.extend(batch)\n",
    "    unique_index = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1445662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Persian BERT, we don't need to lowercase (parsbert is already uncased)\n",
    "# Just create direct mapping\n",
    "\n",
    "lower_tokens = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a76cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1638/1638 [00:00<00:00, 59953.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# create the mapping for Persian tokens\n",
    "\n",
    "token_maps = {}\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    token_maps[t] = {'word': word, 'token': lower_tokens.index(t)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1c94be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapper saved to token_fa_maps.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "print('Token mapper saved to %s' % config['dataset_params']['token_maps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=32, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70874215",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
