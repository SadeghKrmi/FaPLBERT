{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1ea7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932930a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc4ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import LoggerType\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AlbertConfig, AlbertModel\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from model import MultiTaskModel\n",
    "from dataloader import build_dataloader\n",
    "from utils import length_to_mask, scan_checkpoint\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d0c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pickle\n",
    "\n",
    "config_path = \"Configs/config_fa.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a7f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(config['dataset_params']['token_maps'], 'rb') as handle:\n",
    "    token_maps = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158bf338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config['dataset_params']['tokenizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e60819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # F0 loss (regression)\n",
    "\n",
    "best_loss = float('inf')  # best test loss\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "loss_train_record = list([])\n",
    "loss_test_record = list([])\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "log_interval = config['log_interval']\n",
    "save_interval = config['save_interval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4fa9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "    \n",
    "    curr_steps = 0\n",
    "    \n",
    "    dataset = load_from_disk(config[\"data_folder\"])\n",
    "\n",
    "    log_dir = config['log_dir']\n",
    "    if not osp.exists(log_dir): os.makedirs(log_dir, exist_ok=True)\n",
    "    shutil.copy(config_path, osp.join(log_dir, osp.basename(config_path)))\n",
    "    \n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = build_dataloader(dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    num_workers=0, \n",
    "                                    dataset_config=config['dataset_params'])\n",
    "\n",
    "    albert_base_configuration = AlbertConfig(**config['model_params'])\n",
    "    \n",
    "    bert = AlbertModel(albert_base_configuration)\n",
    "    bert = MultiTaskModel(bert, \n",
    "                          num_vocab=1 + max([m['token'] for m in token_maps.values()]), \n",
    "                          num_tokens=config['model_params']['vocab_size'],\n",
    "                          hidden_size=config['model_params']['hidden_size'])\n",
    "    \n",
    "    load = True\n",
    "    try:\n",
    "        files = os.listdir(log_dir)\n",
    "        ckpts = []\n",
    "        for f in os.listdir(log_dir):\n",
    "            if f.startswith(\"step_\"): ckpts.append(f)\n",
    "\n",
    "        iters = [int(f.split('_')[-1].split('.')[0]) for f in ckpts if os.path.isfile(os.path.join(log_dir, f))]\n",
    "        iters = sorted(iters)[-1]\n",
    "    except:\n",
    "        iters = 0\n",
    "        load = False\n",
    "    \n",
    "    optimizer = AdamW(bert.parameters(), lr=1e-4)\n",
    "    \n",
    "    accelerator = Accelerator(mixed_precision=config['mixed_precision'], split_batches=True, kwargs_handlers=[ddp_kwargs])\n",
    "    \n",
    "    if load:\n",
    "        checkpoint = torch.load(log_dir + \"/step_\" + str(iters) + \".t7\", map_location='cpu')\n",
    "        state_dict = checkpoint['net']\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "\n",
    "        bert.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "        accelerator.print('Checkpoint loaded.')\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    bert, optimizer, train_loader = accelerator.prepare(\n",
    "        bert, optimizer, train_loader\n",
    "    )\n",
    "\n",
    "    accelerator.print('Start training...')\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    while curr_steps < num_steps:\n",
    "        for _, batch in enumerate(train_loader):        \n",
    "            curr_steps += 1\n",
    "        \n",
    "            words, labels, phonemes, input_lengths, masked_indices = batch\n",
    "            # text_mask = length_to_mask(torch.Tensor(input_lengths))# .to(device)\n",
    "            text_mask = length_to_mask(torch.Tensor(input_lengths).to(accelerator.device))\n",
    "            tokens_pred, words_pred = bert(phonemes, attention_mask=(~text_mask).int())\n",
    "        \n",
    "            loss_vocab = 0\n",
    "            for _s2s_pred, _text_input, _text_length, _masked_indices in zip(words_pred, words, input_lengths, masked_indices):\n",
    "                loss_vocab += criterion(_s2s_pred[:_text_length], \n",
    "                                            _text_input[:_text_length])\n",
    "            loss_vocab /= words.size(0)\n",
    "        \n",
    "            loss_token = 0\n",
    "            sizes = 1\n",
    "            for _s2s_pred, _text_input, _text_length, _masked_indices in zip(tokens_pred, labels, input_lengths, masked_indices):\n",
    "                if len(_masked_indices) > 0:\n",
    "                    _text_input = _text_input[:_text_length][_masked_indices]\n",
    "                    loss_tmp = criterion(_s2s_pred[:_text_length][_masked_indices], \n",
    "                                                _text_input[:_text_length]) \n",
    "                    loss_token += loss_tmp\n",
    "                    sizes += 1\n",
    "            loss_token /= sizes\n",
    "\n",
    "            loss = loss_vocab + loss_token\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            iters = iters + 1\n",
    "            if (iters+1)%log_interval == 0:\n",
    "                accelerator.print ('Step [%d/%d], Loss: %.5f, Vocab Loss: %.5f, Token Loss: %.5f'\n",
    "                        %(iters+1, num_steps, running_loss / log_interval, loss_vocab, loss_token))\n",
    "                running_loss = 0\n",
    "            \n",
    "            if (iters+1)%save_interval == 0:\n",
    "                accelerator.print('Saving..')\n",
    "\n",
    "                state = {\n",
    "                    'net':  bert.state_dict(),\n",
    "                    'step': iters,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                }\n",
    "\n",
    "                accelerator.save(state, log_dir + '/step_' + str(iters + 1) + '.t7')\n",
    "\n",
    "            if curr_steps > num_steps:\n",
    "                return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b8958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available. Using 1 GPUs.\n",
      "Launching training on one GPU.\n",
      "177\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_188290/3897344229.py:102: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
      "  accelerator.print ('Step [%d/%d], Loss: %.5f, Vocab Loss: %.5f, Token Loss: %.5f'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/1500000], Loss: 12.67912, Vocab Loss: 9.68635, Token Loss: 3.04957\n",
      "Step [20/1500000], Loss: 12.85938, Vocab Loss: 9.62279, Token Loss: 2.96987\n",
      "Step [30/1500000], Loss: 12.11446, Vocab Loss: 9.00001, Token Loss: 2.87786\n",
      "Step [40/1500000], Loss: 11.45519, Vocab Loss: 8.50737, Token Loss: 2.82773\n",
      "Step [50/1500000], Loss: 11.22153, Vocab Loss: 8.67087, Token Loss: 2.80264\n",
      "Step [60/1500000], Loss: 10.82349, Vocab Loss: 8.01265, Token Loss: 2.84067\n",
      "Step [70/1500000], Loss: 10.55812, Vocab Loss: 7.61960, Token Loss: 2.97709\n",
      "Step [80/1500000], Loss: 10.47373, Vocab Loss: 7.33257, Token Loss: 2.61843\n",
      "Step [90/1500000], Loss: 10.62739, Vocab Loss: 7.56134, Token Loss: 2.82019\n",
      "Step [100/1500000], Loss: 10.61979, Vocab Loss: 7.70157, Token Loss: 2.78081\n",
      "Step [110/1500000], Loss: 10.53769, Vocab Loss: 7.28886, Token Loss: 2.81741\n",
      "Step [120/1500000], Loss: 10.42014, Vocab Loss: 7.99653, Token Loss: 2.88146\n",
      "Step [130/1500000], Loss: 10.53974, Vocab Loss: 7.66441, Token Loss: 2.77114\n",
      "Step [140/1500000], Loss: 10.13024, Vocab Loss: 6.89460, Token Loss: 2.76582\n",
      "Step [150/1500000], Loss: 10.44308, Vocab Loss: 7.90308, Token Loss: 2.81228\n",
      "Step [160/1500000], Loss: 10.34771, Vocab Loss: 7.25014, Token Loss: 2.74428\n",
      "Step [170/1500000], Loss: 10.08842, Vocab Loss: 7.76906, Token Loss: 2.76230\n",
      "Step [180/1500000], Loss: 10.27987, Vocab Loss: 8.17401, Token Loss: 2.67460\n",
      "Step [190/1500000], Loss: 10.54886, Vocab Loss: 7.34992, Token Loss: 2.96352\n",
      "Step [200/1500000], Loss: 10.39812, Vocab Loss: 7.71497, Token Loss: 2.71325\n",
      "Step [210/1500000], Loss: 10.20701, Vocab Loss: 7.68008, Token Loss: 2.81221\n",
      "Step [220/1500000], Loss: 10.17066, Vocab Loss: 6.92230, Token Loss: 2.81587\n",
      "Step [230/1500000], Loss: 10.10392, Vocab Loss: 7.32625, Token Loss: 2.62277\n",
      "Step [240/1500000], Loss: 10.33411, Vocab Loss: 7.88814, Token Loss: 2.93586\n",
      "Step [250/1500000], Loss: 10.25439, Vocab Loss: 7.77055, Token Loss: 2.80892\n",
      "Step [260/1500000], Loss: 10.20093, Vocab Loss: 7.55344, Token Loss: 2.69034\n",
      "Step [270/1500000], Loss: 10.13415, Vocab Loss: 6.32819, Token Loss: 2.67539\n",
      "Step [280/1500000], Loss: 10.41637, Vocab Loss: 7.58305, Token Loss: 2.61537\n",
      "Step [290/1500000], Loss: 10.15041, Vocab Loss: 6.74727, Token Loss: 2.58678\n",
      "Step [300/1500000], Loss: 10.09910, Vocab Loss: 7.54436, Token Loss: 2.87186\n",
      "Step [310/1500000], Loss: 9.93880, Vocab Loss: 6.64692, Token Loss: 2.74602\n",
      "Step [320/1500000], Loss: 9.95688, Vocab Loss: 7.65478, Token Loss: 2.67790\n",
      "Step [330/1500000], Loss: 10.14482, Vocab Loss: 7.41841, Token Loss: 2.64937\n",
      "Step [340/1500000], Loss: 10.15315, Vocab Loss: 7.72099, Token Loss: 2.75722\n",
      "Step [350/1500000], Loss: 9.91456, Vocab Loss: 7.66331, Token Loss: 2.66420\n",
      "Step [360/1500000], Loss: 10.10764, Vocab Loss: 7.35404, Token Loss: 2.70055\n",
      "Step [370/1500000], Loss: 9.82380, Vocab Loss: 7.01122, Token Loss: 2.64686\n",
      "Step [380/1500000], Loss: 10.11532, Vocab Loss: 6.77560, Token Loss: 3.15219\n",
      "Step [390/1500000], Loss: 10.17228, Vocab Loss: 8.04417, Token Loss: 2.48601\n",
      "Step [400/1500000], Loss: 10.08309, Vocab Loss: 7.49258, Token Loss: 2.60915\n",
      "Step [410/1500000], Loss: 9.97856, Vocab Loss: 7.45918, Token Loss: 2.57364\n",
      "Step [420/1500000], Loss: 10.05577, Vocab Loss: 7.39755, Token Loss: 2.87802\n",
      "Step [430/1500000], Loss: 9.90089, Vocab Loss: 7.74534, Token Loss: 2.85639\n",
      "Step [440/1500000], Loss: 9.94005, Vocab Loss: 6.93773, Token Loss: 2.79186\n",
      "Step [450/1500000], Loss: 9.71625, Vocab Loss: 7.07686, Token Loss: 2.89121\n",
      "Step [460/1500000], Loss: 10.00359, Vocab Loss: 7.37445, Token Loss: 2.60240\n",
      "Step [470/1500000], Loss: 9.77681, Vocab Loss: 6.10208, Token Loss: 2.70782\n",
      "Step [480/1500000], Loss: 9.65155, Vocab Loss: 6.49395, Token Loss: 2.89280\n",
      "Step [490/1500000], Loss: 9.78307, Vocab Loss: 7.34698, Token Loss: 2.57590\n",
      "Step [500/1500000], Loss: 9.88573, Vocab Loss: 7.24752, Token Loss: 3.27704\n",
      "Step [510/1500000], Loss: 10.05592, Vocab Loss: 7.53066, Token Loss: 2.86552\n",
      "Step [520/1500000], Loss: 9.94853, Vocab Loss: 7.14461, Token Loss: 2.69885\n",
      "Step [530/1500000], Loss: 9.67867, Vocab Loss: 7.07474, Token Loss: 2.80640\n",
      "Step [540/1500000], Loss: 9.75369, Vocab Loss: 7.70900, Token Loss: 2.82980\n",
      "Step [550/1500000], Loss: 9.70035, Vocab Loss: 7.13144, Token Loss: 2.60350\n",
      "Step [560/1500000], Loss: 9.71979, Vocab Loss: 6.94031, Token Loss: 2.63961\n",
      "Step [570/1500000], Loss: 9.70641, Vocab Loss: 6.81893, Token Loss: 2.75233\n",
      "Step [580/1500000], Loss: 9.56992, Vocab Loss: 6.54474, Token Loss: 2.62918\n",
      "Step [590/1500000], Loss: 9.78317, Vocab Loss: 7.15851, Token Loss: 2.43128\n",
      "Step [600/1500000], Loss: 9.77987, Vocab Loss: 7.39607, Token Loss: 2.28656\n",
      "Step [610/1500000], Loss: 9.53175, Vocab Loss: 6.09812, Token Loss: 2.78098\n",
      "Step [620/1500000], Loss: 9.35516, Vocab Loss: 6.69686, Token Loss: 2.64850\n",
      "Step [630/1500000], Loss: 9.59132, Vocab Loss: 7.03656, Token Loss: 2.68386\n",
      "Step [640/1500000], Loss: 9.66373, Vocab Loss: 7.14459, Token Loss: 2.83855\n",
      "Step [650/1500000], Loss: 9.79312, Vocab Loss: 6.37735, Token Loss: 2.83786\n",
      "Step [660/1500000], Loss: 9.64314, Vocab Loss: 6.16959, Token Loss: 2.30660\n",
      "Step [670/1500000], Loss: 9.83391, Vocab Loss: 7.78703, Token Loss: 2.38153\n",
      "Step [680/1500000], Loss: 9.41933, Vocab Loss: 6.83283, Token Loss: 2.54894\n",
      "Step [690/1500000], Loss: 9.44843, Vocab Loss: 7.41029, Token Loss: 2.54631\n",
      "Step [700/1500000], Loss: 9.47565, Vocab Loss: 6.68671, Token Loss: 2.50412\n",
      "Step [710/1500000], Loss: 9.67862, Vocab Loss: 6.75571, Token Loss: 2.49390\n",
      "Step [720/1500000], Loss: 9.24132, Vocab Loss: 5.84960, Token Loss: 2.71480\n",
      "Step [730/1500000], Loss: 9.69624, Vocab Loss: 6.99078, Token Loss: 2.61632\n",
      "Step [740/1500000], Loss: 9.17502, Vocab Loss: 5.89653, Token Loss: 2.41881\n",
      "Step [750/1500000], Loss: 9.53853, Vocab Loss: 6.86434, Token Loss: 2.64450\n",
      "Step [760/1500000], Loss: 9.46881, Vocab Loss: 7.33339, Token Loss: 2.58248\n",
      "Step [770/1500000], Loss: 9.58496, Vocab Loss: 6.98470, Token Loss: 2.73733\n",
      "Step [780/1500000], Loss: 9.19599, Vocab Loss: 6.37959, Token Loss: 2.36621\n",
      "Step [790/1500000], Loss: 9.33874, Vocab Loss: 7.04254, Token Loss: 2.50338\n",
      "Step [800/1500000], Loss: 9.57184, Vocab Loss: 6.91140, Token Loss: 2.55995\n",
      "Step [810/1500000], Loss: 9.21748, Vocab Loss: 6.86230, Token Loss: 2.54113\n",
      "Step [820/1500000], Loss: 9.44419, Vocab Loss: 6.85157, Token Loss: 2.60529\n",
      "Step [830/1500000], Loss: 9.47795, Vocab Loss: 7.06673, Token Loss: 2.36068\n",
      "Step [840/1500000], Loss: 9.35785, Vocab Loss: 6.79608, Token Loss: 2.59215\n",
      "Step [850/1500000], Loss: 9.45920, Vocab Loss: 7.18162, Token Loss: 2.80736\n",
      "Step [860/1500000], Loss: 9.39339, Vocab Loss: 6.44977, Token Loss: 2.61514\n",
      "Step [870/1500000], Loss: 9.44135, Vocab Loss: 6.78909, Token Loss: 2.79300\n",
      "Step [880/1500000], Loss: 9.24431, Vocab Loss: 6.82230, Token Loss: 2.51102\n",
      "Step [890/1500000], Loss: 9.21858, Vocab Loss: 6.71621, Token Loss: 2.86450\n",
      "Step [900/1500000], Loss: 9.35719, Vocab Loss: 6.19505, Token Loss: 2.30523\n",
      "Step [910/1500000], Loss: 9.52080, Vocab Loss: 7.09851, Token Loss: 2.28868\n",
      "Step [920/1500000], Loss: 9.40020, Vocab Loss: 6.07982, Token Loss: 2.68456\n",
      "Step [930/1500000], Loss: 9.55825, Vocab Loss: 7.29692, Token Loss: 2.77112\n",
      "Step [940/1500000], Loss: 9.34307, Vocab Loss: 7.30057, Token Loss: 2.55689\n",
      "Step [950/1500000], Loss: 9.22361, Vocab Loss: 6.97352, Token Loss: 2.56941\n",
      "Step [960/1500000], Loss: 9.42362, Vocab Loss: 7.38290, Token Loss: 2.72565\n",
      "Step [970/1500000], Loss: 9.07648, Vocab Loss: 6.38028, Token Loss: 2.25667\n",
      "Step [980/1500000], Loss: 9.21584, Vocab Loss: 6.09458, Token Loss: 2.60065\n",
      "Step [990/1500000], Loss: 9.36129, Vocab Loss: 6.70932, Token Loss: 2.50940\n",
      "Step [1000/1500000], Loss: 9.26212, Vocab Loss: 5.90506, Token Loss: 2.70654\n",
      "Step [1010/1500000], Loss: 9.39739, Vocab Loss: 6.46609, Token Loss: 2.12122\n",
      "Step [1020/1500000], Loss: 9.08640, Vocab Loss: 5.83262, Token Loss: 2.75254\n",
      "Step [1030/1500000], Loss: 9.18237, Vocab Loss: 7.13533, Token Loss: 2.73189\n",
      "Step [1040/1500000], Loss: 9.29299, Vocab Loss: 6.54052, Token Loss: 2.53978\n",
      "Step [1050/1500000], Loss: 9.31121, Vocab Loss: 7.24768, Token Loss: 2.53785\n",
      "Step [1060/1500000], Loss: 9.59302, Vocab Loss: 7.29019, Token Loss: 2.61441\n",
      "Step [1070/1500000], Loss: 9.23287, Vocab Loss: 6.84804, Token Loss: 2.50605\n",
      "Step [1080/1500000], Loss: 8.93558, Vocab Loss: 6.98519, Token Loss: 2.28585\n",
      "Step [1090/1500000], Loss: 9.12848, Vocab Loss: 6.15118, Token Loss: 2.51015\n",
      "Step [1100/1500000], Loss: 9.40482, Vocab Loss: 7.44851, Token Loss: 2.56667\n",
      "Step [1110/1500000], Loss: 9.38047, Vocab Loss: 7.16546, Token Loss: 2.45225\n",
      "Step [1120/1500000], Loss: 9.31935, Vocab Loss: 6.73859, Token Loss: 2.40394\n",
      "Step [1130/1500000], Loss: 8.97256, Vocab Loss: 6.87545, Token Loss: 2.68223\n",
      "Step [1140/1500000], Loss: 9.12642, Vocab Loss: 6.30258, Token Loss: 2.14071\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_processes = torch.cuda.device_count()\n",
    "    print(f\"CUDA available. Using {num_processes} GPUs.\")\n",
    "else:\n",
    "    num_processes = 1\n",
    "    print(\"CUDA not available. Using 1 CPU process.\")\n",
    "\n",
    "while True:\n",
    "    notebook_launcher(train, args=(), num_processes=num_processes, use_port=33389)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf4988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
